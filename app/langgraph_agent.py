# ----------------------------
# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ----------------------------
import os
import boto3
import time
import json
import random
from typing_extensions import Annotated, TypedDict
from dotenv import load_dotenv
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import AnyMessage, add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, ToolMessage, HumanMessage
from langchain_aws import ChatBedrock
from langgraph.checkpoint.memory import MemorySaver

# ----------------------------
# Claudeç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¦‚ç®—ã‚«ã‚¦ãƒ³ãƒˆ
# ----------------------------
def count_tokens(text: str) -> int:
    # Claude 3 Haiku æƒ³å®šã§1å˜èªâ‰’1.5ãƒˆãƒ¼ã‚¯ãƒ³æ›ç®—ï¼ˆè¶…ç°¡æ˜“ï¼‰
    return int(len(text) / 4) + 1

def truncate_messages(messages, max_tokens=1000):
    total = 0
    result = []
    for m in reversed(messages):
        tokens = count_tokens(m.content)
        if total + tokens <= max_tokens:
            result.insert(0, m)
            total += tokens
        else:
            break
    return result

# ----------------------------
# Yahoo APIç”¨ã®StructuredToolã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ----------------------------
from app.tools.yahoo_tool_wrappers import (
    search_yahoo_products_with_filters_tool
)

# ----------------------------
# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
# ----------------------------
dotenv_path = os.path.join(os.path.dirname(__file__), "..", ".env")
load_dotenv(dotenv_path)

# ----------------------------
# AWS Bedrockã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
# ----------------------------
bedrock_client = boto3.client(
    service_name="bedrock-runtime",
    region_name=os.getenv("BEDROCK_AWS_REGION", "us-east-1"),
    aws_access_key_id=os.getenv("BEDROCK_AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("BEDROCK_AWS_SECRET_ACCESS_KEY"),
)

# ----------------------------
# Claude (Bedrock) è¨­å®š
# ----------------------------
llm = ChatBedrock(
    model=os.getenv("BEDROCK_MODEL_ID", "anthropic.claude-3-haiku-20240307-v1:0"),
    client=bedrock_client,
    temperature=0.7,
    max_tokens=512,  # å®‰å…¨ç¯„å›²ã«æŠ‘ãˆã‚‹
    model_kwargs={
        "system": """
ã‚ãªãŸã¯ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚åº—é ­ã§ãŠå®¢æ§˜ã‚’ãŠè¿ãˆã™ã‚‹ã‚ˆã†ãªæ°—æŒã¡ã§ã€è¦ªåˆ‡ã§ä¸å¯§ãªå¯¾å¿œã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
ãŠå®¢æ§˜ã®ã”è¦æœ›ã«ãŠå¿œãˆã™ã‚‹éš›ã¯ã€å¿…ãšä»¥ä¸‹ã®å°‚ç”¨ãƒ„ãƒ¼ãƒ«ã‚’ã”åˆ©ç”¨ãã ã•ã„ï¼š
- yahoo_search_with_filtersï¼ˆæ¨å¥¨ï¼‰
"""
    }
)

# ----------------------------
# LangGraphã§ä½¿ã†ã‚¹ãƒ†ãƒ¼ãƒˆå®šç¾©
# ----------------------------
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

# ----------------------------
# Claudeã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å±¥æ­´ã‚’æ¸¡ã™ãƒãƒ¼ãƒ‰
# ----------------------------
def llm_node(state: State):
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€å¿…ãšStructuredToolã‚’ä½¿ã£ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚"),
        MessagesPlaceholder(variable_name="messages")
    ])
    agent = prompt | llm.bind_tools([
        search_yahoo_products_with_filters_tool
    ])
    result = agent.invoke(state)
    return {"messages": result}

# ----------------------------
# StructuredToolãƒãƒ¼ãƒ‰å®šç¾©
# ----------------------------
tool_node = ToolNode([
    search_yahoo_products_with_filters_tool
])

# ----------------------------
# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ¡ãƒ¢ãƒªå®šç¾©
# ----------------------------
memory = MemorySaver()

# ----------------------------
# ã‚°ãƒ©ãƒ•æ§‹ç¯‰é–¢æ•°
# ----------------------------
def build_graph():
    graph = StateGraph(State)
    graph.add_node("llm_agent", llm_node)
    graph.add_node("tool", tool_node)
    graph.add_edge(START, "llm_agent")
    graph.add_conditional_edges("llm_agent", tools_condition, {
        "tools": "tool",
        "__end__": END
    })
    graph.add_edge("tool", END)
    return graph.compile(checkpointer=memory)

graph_app = build_graph()

# ----------------------------
# ã‚°ãƒ©ãƒ•ã‚’éåŒæœŸå®Ÿè¡Œã™ã‚‹é–¢æ•°
# ----------------------------
async def run_agent(user_input: str, thread_id: str = "default") -> dict:
    checkpoint = memory.get({"configurable": {"thread_id": thread_id}})
    past_messages = checkpoint.get("state", {}).get("messages", []) if checkpoint else []

    # ğŸ”’ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ä»˜ãã§å±¥æ­´ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°
    limited_past = truncate_messages(past_messages)
    limited_past.append(HumanMessage(content=user_input))
    human_messages = limited_past

    def run_with_retry():
        delay = 1
        for _ in range(5):
            try:
                return list(graph_app.stream(
                    {"messages": human_messages},
                    {"configurable": {"thread_id": thread_id}},
                ))
            except Exception as e:
                if "ThrottlingException" in str(e):
                    time.sleep(delay + random.uniform(0, 0.5))
                    delay *= 2
                else:
                    raise e
        raise RuntimeError("Claude API throttled after multiple retries.")

    complete_raw_events = []
    parsed_tool_content = None

    try:
        for event in run_with_retry():
            complete_raw_events.append(event)
            if "tool" in event:
                for msg in event["tool"].get("messages", []):
                    try:
                        parsed_tool_content = json.loads(msg.content)
                    except Exception:
                        parsed_tool_content = msg.content
    except Exception as e:
        return {"response": {"error": str(e)}}

    return {
        "complete_raw_events": complete_raw_events,
        "parsed_tool_content": parsed_tool_content
    }

# ----------------------------
# ç¾åœ¨ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—
# ----------------------------
def get_memory_state(thread_id: str):
    return memory.get({"configurable": {"thread_id": thread_id}})
