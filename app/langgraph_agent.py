# langgraph_agent.py

import os
import boto3
from dotenv import load_dotenv
from typing_extensions import Annotated, TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import AnyMessage, add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_aws import ChatBedrock
from langchain_core.messages import AIMessage, ToolMessage, HumanMessage
from langgraph.checkpoint.memory import MemorySaver
import json

# âœ… æ¥½å¤©APIã®StructuredToolãƒ©ãƒƒãƒ‘ãƒ¼
from app.tools.rakuten_tool_wrappers import (
    search_products_with_filters_tool,
    keyword_to_ranking_products_tool
)

# -------------------------
# âœ… ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ & Bedrockè¨­å®š
# -------------------------
dotenv_path = os.path.join(os.path.dirname(__file__), "..", ".env")
load_dotenv(dotenv_path)

bedrock_client = boto3.client(
    service_name="bedrock-runtime",
    region_name=os.getenv("BEDROCK_AWS_REGION", "us-east-1"),
    aws_access_key_id=os.getenv("BEDROCK_AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("BEDROCK_AWS_SECRET_ACCESS_KEY"),
)

# ğŸ¤– Claude 3.5ï¼ˆToolä½¿ç”¨ã‚’å¼·åˆ¶ï¼‰
llm = ChatBedrock(
    model=os.getenv("BEDROCK_MODEL_ID", "anthropic.claude-3-haiku-20240307-v1:0"),
    client=bedrock_client,
    temperature=0.7,
    max_tokens=512,  # â† ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°é˜²æ­¢ã®ãŸã‚ã«è¿½åŠ 
    model_kwargs={    # â† systemãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã“ã“ã«ç§»å‹•
        "system": """
ã‚ãªãŸã¯æ¥½å¤©å¸‚å ´ã®ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚åº—é ­ã§ãŠå®¢æ§˜ã‚’ãŠè¿ãˆã™ã‚‹ã‚ˆã†ãªæ°—æŒã¡ã§ã€è¦ªåˆ‡ã§ä¸å¯§ãªå¯¾å¿œã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚

ãŠå®¢æ§˜ã®ã”è¦æœ›ã«ãŠå¿œãˆã™ã‚‹éš›ã¯ã€å¿…ãšä»¥ä¸‹ã®å°‚ç”¨ãƒ„ãƒ¼ãƒ«ã‚’ã”åˆ©ç”¨ãã ã•ã„ï¼š
- rakuten_search_with_filtersï¼ˆæ¨å¥¨ï¼‰
- rakuten_ranking_from_keyword

â€»å¤–éƒ¨ã®æƒ…å ±ã‚„ç‹¬è‡ªã®çŸ¥è­˜ã§ãŠç­”ãˆã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã®ã§ã”äº†æ‰¿ãã ã•ã„ã€‚

ã€ãŠå®¢æ§˜ã¸ã®ã”æ¡ˆå†…ã®ãƒã‚¤ãƒ³ãƒˆã€‘ï¼š
æ¤œç´¢çµæœã‚’ã‚‚ã¨ã«ã€å•†å“ã®ç‰¹å¾´ã‚„ä¾¡æ ¼å¸¯ã‚’åˆ†ã‹ã‚Šã‚„ã™ãã”èª¬æ˜ã—ã¾ã™ã€‚
ä»¥ä¸‹ã®ç‚¹ã«ã”é…æ…®ãã ã•ã„ï¼š
- ã€Œ2000å††å‰å¾Œã®ä½¿ã„ã‚„ã™ã„ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³ãŒäººæ°—ã§ã™ã€ãªã©ã€ç‰¹å¾´ã‚’ã¾ã¨ã‚ã¦ãŠä¼ãˆã—ã¾ã™
- å•†å“åã‚„å‹ç•ªã‚’ãã®ã¾ã¾èª­ã¿ä¸Šã’ã‚‹ã“ã¨ã¯ã„ãŸã—ã¾ã›ã‚“
- ç°¡æ½”ã«1ã€œ2æ–‡ã§è¦ç‚¹ã‚’ãŠä¼ãˆã—ã€ãŠå®¢æ§˜ãŒè¿·ã‚ãªã„ã‚ˆã†ã«å¿ƒãŒã‘ã¾ã™

ä½•ã‹ãŠæ¢ã—ã®å•†å“ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠç”³ã—ä»˜ã‘ãã ã•ã„ï¼
"""
    }
)

# -------------------------
# ğŸ’¾ ã‚¹ãƒ†ãƒ¼ãƒˆå®šç¾©
# -------------------------
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

# -------------------------
# ğŸ¤– LLMãƒãƒ¼ãƒ‰
# -------------------------
def llm_node(state: State):
    prompt = ChatPromptTemplate.from_messages([
        ("system", "æ¥½å¤©ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€å¿…ãšStructuredToolã‚’ä½¿ã£ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚"),
        MessagesPlaceholder(variable_name="messages")
    ])
    agent = prompt | llm.bind_tools([
        search_products_with_filters_tool,
        keyword_to_ranking_products_tool
    ])
    result = agent.invoke(state)
    return {"messages": result}

# -------------------------
# ğŸ§° Toolãƒãƒ¼ãƒ‰
# -------------------------
tool_node = ToolNode([
    search_products_with_filters_tool,
    keyword_to_ranking_products_tool
])

# -------------------------
# ğŸ§  LangGraphæ§‹ç¯‰
# -------------------------
def build_graph():
    graph = StateGraph(State)

    graph.add_node("llm_agent", llm_node)
    graph.add_node("tool", tool_node)

    graph.add_edge(START, "llm_agent")
    graph.add_conditional_edges("llm_agent", tools_condition, {
        "tools": "tool",
        "__end__": END
    })
    graph.add_edge("tool", "llm_agent")
    graph.add_edge("llm_agent", END)

    # âœ… ãƒ¡ãƒ¢ãƒªä¿å­˜æ©Ÿèƒ½ã‚’è¿½åŠ 
    return graph.compile(checkpointer=memory)

# -------------------------
# ğŸš€ å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
# -------------------------
if __name__ == "__main__":
    app = build_graph()
    print("ğŸ›ï¸ Shoppieã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã‚ˆã†ã“ãï¼")
    user_input = input("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè©±ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š")
    events = app.stream({"messages": [("user", user_input)]})
    for event in events:
        print(event)

# -------------------------
# ğŸŒ FastAPIã‹ã‚‰ä½¿ã†é–¢æ•°
# -------------------------
async def run_agent(user_input: str, thread_id: str = "default") -> dict:
    app = build_graph()

    # âœ… thread_id ã«åŸºã¥ã„ã¦çŠ¶æ…‹ã‚’ç¶™ç¶š
    events = app.stream(
        {"messages": [HumanMessage(content=user_input)]},
        {"configurable": {"thread_id": thread_id}},
    )

    complete_raw_events = []
    parsed_tool_content = None

    for event in events:
        complete_raw_events.append(event)
        if "tool" in event:
            for msg in event["tool"].get("messages", []):
                if isinstance(msg, ToolMessage):
                    try:
                        parsed_tool_content = json.loads(msg.content)
                    except Exception:
                        parsed_tool_content = msg.content

    return {
        "complete_raw_events": complete_raw_events,
        "parsed_tool_content": parsed_tool_content
    }

# âœ… MemorySaver ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆï¼ˆã‚¢ãƒ—ãƒªå…¨ä½“ã§å…±é€šã«ä½¿ç”¨ï¼‰
memory = MemorySaver()