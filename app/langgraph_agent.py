import os
import boto3
from dotenv import load_dotenv
from typing_extensions import Annotated, TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import AnyMessage, add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_aws import ChatBedrock
from langchain_core.messages import AIMessage, ToolMessage, HumanMessage
from langgraph.checkpoint.memory import MemorySaver
import json
from langgraph.checkpoint.base import Checkpoint

# âœ… æ¥½å¤©APIã®StructuredToolãƒ©ãƒƒãƒ‘ãƒ¼
from app.tools.rakuten_tool_wrappers import (
    search_products_with_filters_tool,
    keyword_to_ranking_products_tool
)

# -------------------------
# âœ… ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ & Bedrockè¨­å®š
# -------------------------
dotenv_path = os.path.join(os.path.dirname(__file__), "..", ".env")
load_dotenv(dotenv_path)

bedrock_client = boto3.client(
    service_name="bedrock-runtime",
    region_name=os.getenv("BEDROCK_AWS_REGION", "us-east-1"),
    aws_access_key_id=os.getenv("BEDROCK_AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("BEDROCK_AWS_SECRET_ACCESS_KEY"),
)

# ğŸ¤– Claude 3.5ï¼ˆToolä½¿ç”¨ã‚’å¼·åˆ¶ï¼‰
llm = ChatBedrock(
    model=os.getenv("BEDROCK_MODEL_ID", "anthropic.claude-3-haiku-20240307-v1:0"),
    client=bedrock_client,
    temperature=0.7,
    max_tokens=256,
    model_kwargs={
        "system": """
ã‚ãªãŸã¯æ¥½å¤©å¸‚å ´ã®ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚åº—é ­ã§ãŠå®¢æ§˜ã‚’ãŠè¿ãˆã™ã‚‹ã‚ˆã†ãªæ°—æŒã¡ã§ã€è¦ªåˆ‡ã§ä¸å¯§ãªå¯¾å¿œã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚

ãŠå®¢æ§˜ã®ã”è¦æœ›ã«ãŠå¿œãˆã™ã‚‹éš›ã¯ã€å¿…ãšä»¥ä¸‹ã®å°‚ç”¨ãƒ„ãƒ¼ãƒ«ã‚’ã”åˆ©ç”¨ãã ã•ã„ï¼š
- rakuten_search_with_filtersï¼ˆæ¨å¥¨ï¼‰
- rakuten_ranking_from_keyword

â€»å¤–éƒ¨ã®æƒ…å ±ã‚„ç‹¬è‡ªã®çŸ¥è­˜ã§ãŠç­”ãˆã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã®ã§ã”äº†æ‰¿ãã ã•ã„ã€‚

ã€ãŠå®¢æ§˜ã¸ã®ã”æ¡ˆå†…ã®ãƒã‚¤ãƒ³ãƒˆã€‘ï¼š
æ¤œç´¢çµæœã‚’ã‚‚ã¨ã«ã€å•†å“ã®ç‰¹å¾´ã‚„ä¾¡æ ¼å¸¯ã‚’åˆ†ã‹ã‚Šã‚„ã™ãã”èª¬æ˜ã—ã¾ã™ã€‚
ä»¥ä¸‹ã®ç‚¹ã«ã”é…æ…®ãã ã•ã„ï¼š
- ã€Œ2000å††å‰å¾Œã®ä½¿ã„ã‚„ã™ã„ãƒ¯ã‚¤ãƒ¤ãƒ¬ã‚¹ã‚¤ãƒ¤ãƒ›ãƒ³ãŒäººæ°—ã§ã™ã€ãªã©ã€ç‰¹å¾´ã‚’ã¾ã¨ã‚ã¦ãŠä¼ãˆã—ã¾ã™
- å•†å“åã‚„å‹ç•ªã‚’ãã®ã¾ã¾èª­ã¿ä¸Šã’ã‚‹ã“ã¨ã¯ã„ãŸã—ã¾ã›ã‚“
- ç°¡æ½”ã«1ã€œ2æ–‡ã§è¦ç‚¹ã‚’ãŠä¼ãˆã—ã€ãŠå®¢æ§˜ãŒè¿·ã‚ãªã„ã‚ˆã†ã«å¿ƒãŒã‘ã¾ã™

ä½•ã‹ãŠæ¢ã—ã®å•†å“ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã€ãŠæ°—è»½ã«ãŠç”³ã—ä»˜ã‘ãã ã•ã„ï¼
"""
    }
)

# -------------------------
# ğŸ’¾ ã‚¹ãƒ†ãƒ¼ãƒˆå®šç¾©
# -------------------------
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

# -------------------------
# ğŸ¤– LLMãƒãƒ¼ãƒ‰
# -------------------------
def llm_node(state: State):
    prompt = ChatPromptTemplate.from_messages([
        ("system", "æ¥½å¤©ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€å¿…ãšStructuredToolã‚’ä½¿ã£ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚"),
        MessagesPlaceholder(variable_name="messages")
    ])
    agent = prompt | llm.bind_tools([
        search_products_with_filters_tool,
        keyword_to_ranking_products_tool
    ])
    result = agent.invoke(state)
    return {"messages": result}

# -------------------------
# ğŸ§° Toolãƒãƒ¼ãƒ‰
# -------------------------
tool_node = ToolNode([
    search_products_with_filters_tool,
    keyword_to_ranking_products_tool
])

# -------------------------
# ğŸ’¾ MemorySaverã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼‰
# -------------------------
memory = MemorySaver()

# -------------------------
# ğŸŒ LangGraphæ§‹ç¯‰é–¢æ•°
# -------------------------
def build_graph():
    graph = StateGraph(State)
    graph.add_node("llm_agent", llm_node)
    graph.add_node("tool", tool_node)
    graph.add_edge(START, "llm_agent")
    graph.add_conditional_edges("llm_agent", tools_condition, {
        "tools": "tool",
        "__end__": END
    })
    graph.add_edge("tool", "llm_agent")
    graph.add_edge("llm_agent", END)
    return graph.compile(checkpointer=memory)

# âœ… ã‚°ãƒ©ãƒ•ã‚’ä¸€åº¦ã ã‘æ§‹ç¯‰
graph_app = build_graph()

# -------------------------
# ğŸš€ å®Ÿè¡Œé–¢æ•°ï¼ˆFastAPIãªã©ã‹ã‚‰å‘¼ã°ã‚Œã‚‹ï¼‰
# -------------------------
async def run_agent(user_input: str, thread_id: str = "default") -> dict:
    # æ—¢å­˜ãƒ¡ãƒ¢ãƒªã‹ã‚‰å–å¾—ï¼ˆstateã«messagesãŒãªã‘ã‚Œã°ç©ºãƒªã‚¹ãƒˆï¼‰
    checkpoint = memory.get({"configurable": {"thread_id": thread_id}})
    past_messages = checkpoint.get("state", {}).get("messages", []) if checkpoint else []

    # HumanMessage ã®ã¿æŠ½å‡º
    human_messages = [m for m in past_messages if isinstance(m, HumanMessage)]
    human_messages.append(HumanMessage(content=user_input))

    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œï¼ˆHumanMessageã®ã¿æ¸¡ã™ï¼‰
    events = graph_app.stream(
        {"messages": human_messages},
        {"configurable": {"thread_id": thread_id}},
    )

    complete_raw_events = []
    parsed_tool_content = None

    for event in events:
        complete_raw_events.append(event)
        if "tool" in event:
            for msg in event["tool"].get("messages", []):
                try:
                    parsed_tool_content = json.loads(msg.content)
                except Exception:
                    parsed_tool_content = msg.content

    return {
        "complete_raw_events": complete_raw_events,
        "parsed_tool_content": parsed_tool_content
    }



# -------------------------
# âœ… ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹å–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -------------------------
def get_memory_state(thread_id: str):
    return memory.get({"configurable": {"thread_id": thread_id}})
